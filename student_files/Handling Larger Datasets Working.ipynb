{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Handling Larger Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "A profiling decorator.  Simply place the decorator above a function that takes a long time.  Then call the function.  Afterwards, examine the function's **time_results** attribute to see (in seconds) how long it took to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile(orig):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        ret = orig(*args, **kwargs)\n",
    "        finish = time.time()\n",
    "        wrapper.time_results = (finish - start)\n",
    "        return ret\n",
    "    wrapper.time_results = 0.0\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "A function to read (somewhat generically) from a file.  Since it is decorated, it can help us determine how long the function takes by checking **load_data.time_results**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def load_data(filename, sep=',', names=None, usecols=None, header='infer', nrows=None, skiprows=None):\n",
    "    data = pd.read_csv(filename, sep=sep, header=header, low_memory=False, \n",
    "                       names=names, usecols=usecols, nrows=nrows, skiprows=skiprows)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Our raw dataset loads into memory, but let's examine resources utilized...\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path('helpfulness_reviews.csv')\n",
    "if not filename.exists():\n",
    "    filename = Path('helpfulness_reviews_smaller.csv')\n",
    "    \n",
    "if not filename.exists():\n",
    "    print('File does not exist.  Can not continue.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(filename)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data.time_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!! python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "#### **Use Caution with Various Pandas Operations**\n",
    "Simple operations, such as simply renaming a column, can create entirely new copies of DataFrames, thus incurring large memory hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.rename(columns={'ProductId': 'Product_ID'})\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we should have done was use **inplace=True** to avoid creating a new copy.  Notice below that df and new_df have different IDs.  This means they at different locations in memory.  Both occupy 40Mb of memory.  Both are globally defined and therefore will stay until the Python Virtual Machines shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id(df), id(new_df), sys.getsizeof(df), sys.getsizeof(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "#### Limiting the Feature Set Read\n",
    "With large datasets, you should limit the feature set to only columns needed (**usecols=()**)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(filename, usecols=(0, 1, 4, 5, 6))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data.time_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version consumes about half the resources and about half the time to load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "#### Limiting the Number of Rows Read\n",
    "Through the use of **skiprows** and **nrows**, we can limit our reads also...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(filename, usecols=(0, 1, 4, 5, 6), nrows=250000)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data.time_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(filename, usecols=(0, 1, 4, 5, 6), names=names,\n",
    "               nrows=250000, skiprows=250000, header=None)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data.time_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "#### Downcasting\n",
    "Downcasting can reduce the sizes of your in-memory data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.dropna(subset=['embark_town'], inplace=True) \n",
    "titanic.age.fillna(titanic.age.median(), inplace=True)\n",
    "titanic.drop(labels=['deck'], inplace=True, axis=1)\n",
    "titanic.replace({'embarked': {'C': 0, 'Q': 1, 'S': 2},\n",
    "                 'who': {'man': 0, 'woman': 1, 'child': 2}}, inplace=True)\n",
    "titanic.embark_town = titanic.embark_town.astype('category')\n",
    "titanic.embark_town = titanic.embark_town.cat.codes\n",
    "titanic = pd.get_dummies(titanic, columns=['class'])\n",
    "titanic.drop(labels=['alive', 'alone', 'adult_male', 'sex'], inplace=True, axis=1)\n",
    "# lb = LabelBinarizer()\n",
    "# pclass_encoded = pd.DataFrame(lb.fit_transform(titanic.pclass))\n",
    "# titanic = pd.concat([titanic, pclass_encoded], axis=1, join='inner')\n",
    "# titanic.drop('pclass', axis=1, inplace=True)\n",
    "titanic.fare = StandardScaler().fit_transform(titanic.fare.to_numpy().reshape(-1, 1))\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.age = titanic.age.astype(np.float16)\n",
    "titanic.fare = titanic.fare.astype(np.float16)\n",
    "titanic.survived = titanic.survived.astype(np.int8)\n",
    "titanic.sibsp = titanic.sibsp.astype(np.int8)\n",
    "titanic.parch = titanic.parch.astype(np.int8)\n",
    "titanic.embarked = titanic.embarked.astype(np.int8)\n",
    "titanic.who = titanic.who.astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use category types for columns when those columns have a limited set of values.  Category types use dictionaries to map values and therefore consume less resources than a column containing many repeated strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "#### Use Databases\n",
    "Use SQL and the database to either stage data and/or help limit the data read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "#### Use Sampling on Datasets\n",
    "Below, we take a random sub-sample of our overall dataset.  We do this by first defining how big of a sample to take.  We take a 10% sample size.  sample (below) represents this actual number, which for our dataset is 56485 rows.  \n",
    "We then defined rows to skip (randomly).  The syntax for random.sample(population, k) chooses k rows (randomly) from the entire population.  We then use this for the \"skiprows\" argument, thus skipping that many rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_sample_size = 10\n",
    "\n",
    "for idx, line in enumerate(open(filename, encoding='utf-8')):       # Calculate number of rows in file\n",
    "    pass\n",
    "nrows = idx\n",
    "sample = nrows // percent_sample_size                               # a 10% sample size, for us approx. 56485\n",
    "skip = sorted(random.sample(range(1, nrows + 1), nrows - sample))   # defines (random) rows to be skipped  \n",
    "df = pd.read_csv(filename, skiprows=skip)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "#### Load Data in Chunks\n",
    "By loading our data in chunks, we won't be handling everything at once.  Instead, we'll be reading only parts of the data at a time, fitting a model from the partial data and saving that model.  This may lead us to create several models from the several chunks we may read in.  All of this is dependent on what model you are creating.  Some models support partial data available while others need the entire dataset.  \n",
    "\n",
    "The following is valid code except we didn't define our features and label.\n",
    "\n",
    "With this approach, you can work with larger data sizes than you have RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "# use an algorithm of your choosing\n",
    "from sklearn.linear_model import LogisticRegression       \n",
    "\n",
    "chunksize = 100000                                      # how many rows to read at a time\n",
    "models = []\n",
    "for chunk in pd.read_csv(filename, chunksize=chunksize):\n",
    "    # pre-process data\n",
    "    model = LogisticRegression()\n",
    "    model.fit(chunk[features], chunk[label])\n",
    "    models.append(model)\n",
    "\n",
    "predictions = mean([model.predict(features) for model in models], axis=0)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downside to this approach is that if a failure occurs, there is no failover and you must start all over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "#### Work with Dask\n",
    "Another way of handling larger datasets, is by using Dask. Dask parallelizes Numpy and Pandas. Dask Dataframes implement many of the Pandas dataframes methods but Dask processes the data in parallel using multiple cores.  Dask DataFrame APIs are similar to Pandas DataFrames APIs.\n",
    "\n",
    "Dask extends Python data analytics to around 100-500 Gb.  Beyond this, PySpark become the inevitable direction.\n",
    "<br><br>\n",
    "Dask creates a graph to support desired operations.  They are not executed immediately.  When it is time to perform the operations in the graph, the Dask task scheduler partially loads data and runs it through the graph into multiple cores.  Dask also combines the results back at the end and returns any final results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loads multiple data files into a single dataframe of 5+ million records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_time = 0\n",
    "frames = []\n",
    "for count in range(10):\n",
    "    frames.append(load_data(filename))\n",
    "    overall_time += load_data.time_results\n",
    "df = pd.concat(frames, sort=False)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(overall_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It occupies a significant chunk of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our decorator is cool, we can use a magic command to give us timing results too..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_one(df):\n",
    "    df['HelpfulnessNumerator'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "add_one(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_size(df, group_name):\n",
    "    return df.groupby(group_name).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "get_group_size(df, 'Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Dask...\n",
    "<br><br>\n",
    "Dask can assist in parallelization, thus making code run faster and even ease memory concerns.  It can run on a local machine or in a cluster of machines and can run multiple threads or processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, our function runs and (not surprisingly) it takes a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(arg1, arg2):\n",
    "    time.sleep(1)\n",
    "    return arg1 + arg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = [1, 2, 3, 4, 5]\n",
    "for val in data:\n",
    "    print(func1(val, val-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Now, we'll start a Dask server (by default it will run locally).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed, compute\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Dask provides a module called dask.array that behaves like Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "dask_array = da.arange(1, 10)\n",
    "dask_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_array2 = da.random.randint(low=1, high=10, size=1000000000)   # 1 billion element array\n",
    "dask_array2 = dask_array2.reshape(1000000, 1000)\n",
    "dask_array2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_array2.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_array2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result = delayed(func1)(5, 10)\n",
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below runs instantly because it merely creates a graph to be run later.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = [1, 2, 3, 4, 5]\n",
    "results = []\n",
    "delayed_func1 = delayed(func1)\n",
    "for val in data:\n",
    "    results.append(delayed_func1(val, val-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "compute(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Dask provides a DataFrame that behaves like Pandas dataframe but operations can be scheduled and executed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv([filename] * 10, blocksize=64000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute(ddf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_one(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.map_partitions(lambda df: add_one(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dask dataframe is broken into many Pandas dataframes.  These smaller dataframes are called partitions.  You don't control the partition sizes.  The function above is called for each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.memory_usage().sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf = client.persist(ddf)\n",
    "result = ddf.groupby('Score').size().compute()\n",
    "\n",
    "print(type(result))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "print('Client closed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
