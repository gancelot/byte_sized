{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Data Cleansing with Pandas\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "#### Data Cleansing (sometimes called Data Cleaning) is the process of correcting or removing irregular values from a dataset\n",
    "\n",
    " - Tasks involved:\n",
    "- Exploration (Detection)\n",
    "        - Examine the dataset for completeness (e.g., how many missing values)\n",
    "        - Verify constraints (regular expressions, type, ranges, cross-field constraints)\n",
    "- Fix Anomalies (remove and/or replace)\n",
    "        - Remove observations or features containing missing values\n",
    "        - Impute values using different strategies\n",
    "        - Remove duplicates\n",
    "        - Ensure text formatting and units are consistent\n",
    "        - Detect and handle outliers\n",
    "- Verify Changes\n",
    " \n",
    "\n",
    "In addition to the above, if you are data modeling, you will likely need to perform a few additional tasks:\n",
    " - Encode data values\n",
    " - Convert continuous values to categorical\n",
    " - Cut and bin continuous values\n",
    " - Perform type conversions\n",
    " - Normalize / Standarize column values\n",
    "\n",
    "   <br><br>\n",
    "   Before using this notebook, you must install dependencies with:\n",
    "\n",
    "   <code>pip install jupyter pandas matplotlib seaborn</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from datetime import date\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "The Titanic dataset is a good place to start because it can illustrate many of the concepts discussed above. So, we'll use this dataset for examining this topic.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Titanic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Task 1: Data Exploration\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 steps you should *always* perform after acquiring your dataset within Pandas: \n",
    "- shape\n",
    "- head()\n",
    "- info()\n",
    "\n",
    "Do these **often**, even after initial data acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "**dataframe.info()** (above) can identify many things including rows that don't have complete values.\n",
    "<br>\n",
    "You can also use the following statement to help detect which columns don't have values (or have NaN values)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display NaN counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Two of the columns above have missing values (embarked and embark_town).  You can look at these rows with the following statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display rows containing NaNs in embark_town column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "The following shows rows where NaNs appear in *any* column..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display rows containing NaNs in any column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "You can represent NaNs using a heatmap from the data visualization library, called Seaborn.\n",
    "We could do this on multiple columns, but the others are not yet encoded, so we'll just use the 'age' column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display NaNs visually using a Seaborn heatmap\n",
    "# Red represents a NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Pandas Note: *df.isna()*   vs.   *ds.isnull()*\n",
    "<br>\n",
    "In Pandas, these methods are identical.\n",
    "<br><br>\n",
    "Use these methods when necessary:<br>\n",
    "    Numpy...  <code>np.isnan(arr)</code><br>\n",
    "    Pandas...  <code>df.isnull()</code> or <code>df.isna()</code>       \n",
    "    \n",
    "There are 2 ways in Pandas to mimic how it is done in R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "We can also view all the values in these columns to get a better picture of what they contain..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all values found in the embark_town column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all values found in the sibsp column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "(above) The **unique()** function shows all the values that can occur in a specified column.  Including NaNs.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Use **value_counts()** to tell us how many can be found of a certain value in this column.  NaNs are not shown by default.  Also, this only works on a Series (single-column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the value counts from the embark_town column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to actually see the NaNs, use dropna=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the value counts including the NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the value counts for the sibsp column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Validating Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate that all values within the pclass column are either 1, or 2, or 3 (for their class level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Task 2: Fixing Anomalies\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Observations with Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It is possible to remove *all* observations containing NaNs in any column using:\n",
    "<code>titanic.dropna(inplace=True)</code>\n",
    "<br>\n",
    "This might be too big of an operation and may result in an unnecessary loss of data.  We'll try to handle this in a smaller, more fine-grained way, if possible...\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Time to handle the data in the embark_town column...we'll remove the two records where NaNs occur here...\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the 2 records where NaNs occur in the embark_town column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Once again, a check of the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the info() once again..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "The above shows 889 samples are non-null in the embark_town column now.  We can also check for NaNs directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which columns still contain NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### Imputing Values\n",
    "We'll treat the age column next by imputing values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean value in the age column (don't count NaNs by using skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the median value in the age column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "To impute values, you first decide on a strategy (mean, median, some other value).  Then use fillna():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute values within the age column (use the median value from above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the NaN counts again..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "**Scikit-learn**, a Python-based data modeling tool, provides a class that can do this also, called *SimpleImputer*.  However, this solution is not quite as easy to read..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "column_to_impute = titanic[['age']].to_numpy()\n",
    "imputed_column = imputer.fit_transform(column_to_impute)\n",
    "titanic['age'] = pd.DataFrame(imputed_column)\n",
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### Removing Columns\n",
    "The deck has many missing values--perhaps too many to impute anything meaningful.  So, we'll simply drop this column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the deck column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the NaN counts again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### Managing Duplicate Observations\n",
    "Our Titanic dataset doesn't incorporate duplicate observations very well, so we'll use another sample dataset for illustration purposes.  Consider the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_cream = pd.DataFrame({\n",
    "    'flavor': ['Vanilla', 'Chocolate', 'Strawberry', 'Vanilla', 'Vanilla', 'Chocolate', 'Vanilla'],\n",
    "    'topping': ['None', 'Sprinkles', 'Hot Fudge', 'Sprinkles', 'Sprinkles', 'Caramel', 'None'],\n",
    "    'cost': [4, 4.75, 4.75, 4.75, 4.75, 4.75, 4]\n",
    "})\n",
    "ice_cream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "We can use **drop_duplicates()** to remove any observations that have identical values in all of the corresponding columns..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "If we only wish to consider duplication of data in a specified column or columns, use **subset=**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate flavors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### Text Formatting and Pandas Accessors\n",
    "Pandas provides three accessors (str, dt, cat) that allow column operations element-by-element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personnel = pd.DataFrame([('Edward', 'Janitor', date(2004, 3, 3)),\n",
    "                   ('Tracy', 'Teacher', date(2017, 7, 17)),\n",
    "                   ('Amanda', 'Bus Driver', date(2011, 5, 21))],\n",
    "                  columns=['Name', 'Position', 'Hire_date'])\n",
    "personnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uppercase the Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the month of hire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Encoding Values\n",
    "Now for some data encoding...\n",
    "<br><br>\n",
    "First, let's look at the data types for the columns..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the dtypes of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the types using info()..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Columns that are object types are not useful when creating models.  They need to be converted or encoded.\n",
    "The easiest way to encode values is through **simple value-replacement**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the value_counts() of the embarked column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the value_counts() of the 'who' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use value replacement to replace the values in the embarked and who columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the first 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the info() again..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> \n",
    "Now the who and embarked columns are int dtypes.\n",
    "<br><br>\n",
    "For the embark_town column, **we'll use categorical encoding**.  First it needs to be converted to a category type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the value_counts() of the embark_town column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the embark_town column to a category type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the codes attribute of the cat accessor to encode the embark_town column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the value_counts() of embark_town again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the first 5 records of the dataframe again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the info() function to see the column types again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "The class column is a category type, but it contains strings, which can cause problems.  Let's **one-hot encode** this column using Pandas **get_dummies(df, columns)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use get_dummies() to one-hot encode the 'class' column.  Examine the first 5 rows of the resulting dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "The alive column represents similar information to the survived label and the along column represents similar information to the sibsp.  The adult_male and sex columns are also covered by the \"who\" column.  So, **we'll drop these four columns**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the above mentioned columns\n",
    "# view the first 5 rows of the resulting dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Even though we did this using Pandas get_dummies() already, as a demonstration, we'll convert the **pclass** column into a one-hot encoded solution **using Scikit-learn's LabelBinarizer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "pclass_encoded = pd.DataFrame(lb.fit_transform(titanic.pclass), columns=['p1', 'p2', 'p3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.concat([titanic, pclass_encoded], axis=1, join='inner')\n",
    "titanic.drop('pclass', axis=1, inplace=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### Normalization and Standardization\n",
    "For some algorithms, large values can affect results.  Often using normalization and scaling can help with results.\n",
    "<br>\n",
    "We'll use a StandardScaler to change those values.  Examples of StandardScaler and MinMaxScaler...\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Z-score (Standardization) Scaling** - transforms features so that they have a mean = 0, and a std dev. of 1.<br>\n",
    "**Min-Max Scaling (Normalization)** -  Scales values down to fit into a specified range (usually 0 to 1).\n",
    "<br>\n",
    "To help understand each, take the following example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = [0, 1, 2]\n",
    "data = pd.DataFrame(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled = StandardScaler().fit_transform(data)\n",
    "data_scaled = pd.DataFrame(data_scaled)\n",
    "data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_minmax = MinMaxScaler().fit_transform(data)\n",
    "data_minmax = pd.DataFrame(data_minmax)\n",
    "data_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "We'll use Standardization for the fare column..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.fare.to_numpy().reshape(-1, 1)[:5]      # a sample of the first 5 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a standardscaler to scale the values in the fare column.  View the first 5 rows of the resulting df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Outlier Detection Analysis and Removal\n",
    "<br><br>\n",
    "Lastly, we'll perform an outlier detection analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=titanic, x=titanic.age);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=titanic, x=titanic.fare);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_by_column(df, column, whisker=1.5):\n",
    "    Q1 = df[column].quantile(0.25)  # calculate Q1\n",
    "    Q3 = df[column].quantile(0.75)  # calculate Q3\n",
    "    IQR = Q3 - Q1\n",
    "    width = whisker * IQR\n",
    "    lower_bound = Q1 - width\n",
    "    upper_bound = Q3 + width\n",
    "    filter = (df[column] >= lower_bound) &  (df[column] <= upper_bound)\n",
    "    return df.loc[filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers from the 'fare' column\n",
    "# view the resulting shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers from the 'age' column\n",
    "# view the resulting shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with outliers removed, assign the titanic to the value returned from the last function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the first five rows of our df once again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the column structure (using info() again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine if we have any remaining NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsize the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the column structure one last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the dataframe one lst time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
